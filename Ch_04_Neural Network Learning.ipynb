{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch_04_신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 장(3장)에서는 신경망의 가중치를 직접 설정해 둔 값으로 구현을 해보았다.\n",
    "\n",
    "하지만 이번에는 훈련데이터로부터 직접 최적의 결과를 내는 가중치 값을 찾게끔 구현할려고 한다.\n",
    "\n",
    "이때 최적의 결과라는 걸 설명해줄 손실함수(loss function)이란 것을 정의하는데,\n",
    "\n",
    "손실함수의 값이 최소가 되도록 가중치값을 조정하도록 할 것이다.\n",
    "\n",
    "이는 수학에서 최적화 문제에 해당하는데, 최적화 방법중 하나인 gradient method을 소개한다.\n",
    "\n",
    "4.1 데이터에서 학습한다!\n",
    "\n",
    "신경망의 특장점 중 하나는 데이터에 맞게 가중치 매개변수(weight parameter)를 학습한다는 점이다.\n",
    "\n",
    "딥러닝에서는 가중치의 수가 수천, 수만이기 때문에 일일이 수작업으로 지정해주는 건 불가능 하다.\n",
    "\n",
    "이번 단원에서는 MNIST의 데이터에 따라 학습하는 신경망을 구현해보자.\n",
    "\n",
    "4.1.1 데이터 주도 학습\n",
    "\n",
    "\n",
    "기본적으로 기계학습은 데이터로부터 패턴을 발견하고 학습을 함.\n",
    "\n",
    "이전의 기계학습은 데이터로 부터 특징(feature)을 추출해 내어 그 특증의 패턴을 기계학습 알고리즘으로 학습하는 방법이었다.\n",
    "\n",
    "예를 들면 숫자 이미지(그림4-1)로부터 feature를 추출하여 svm이나 k-NN 방법으로 학습을 하는 것이다.\n",
    "\n",
    "하지만 svm이나 k-NN같은 학습을 기계가 하는 부분이라고 하면, 데이터로부터 적절한 feature를 추출하는 것은 사람의 개입하는 정도가 크다.\n",
    "\n",
    "하지만 신경망이나 딥러닝의 경우에는 사람의 개입이 없는(그래서 end-to-end machine learning이라고 불림) 기계학습 방법이다.\n",
    "\n",
    "이런 방법의 이점은 어떤 데이터이든지 같은 맥락에서 기계학습이 가능하다.\n",
    "\n",
    "(이전의 경우에는 숫자데이터에 알맞은 feature, 강아지 사진에 알맞는 feature를 사람이 따로 고안을 했어야 함)\n",
    "\n",
    "그래서 우리는 현재까진 흠이 없어보이는 이 신경망에 대해서 좀 더 알아보자.\n",
    "\n",
    "4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    "신경망을 설명하기 앞서 기계학습 데이터 취급시 주의할 점을 보자.\n",
    "\n",
    "기계학습의 데이터는 training(훈련),test(시험)데이터 2가지로 분류한다.\n",
    "\n",
    "훈련 데이터로 기계학습을 시켜 최적의 가중치를 찾아낸 다음에, 시험 데이터로 그 모델의 성능(범용 능력이라 함)을 평가하는 것이다.\n",
    "\n",
    "그러면 왜 이렇게 학습데이터를 분류를 해야하는가?\n",
    "\n",
    "모델의 궁극적인 목표는 새로운 데이터에 대해 어떤 성능을 발휘하는 가이다.\n",
    "\n",
    "그런데 학습데이터에만 잘 들어맞는 모델은 실제로 쓸모가 없다. 또 학습데이터에만 지나치게 모델이 잘 들어맞는 것을 overfitting(과적합)이라고 한다.\n",
    "\n",
    "이 오버피팅을 피하기 위해 학습데이터를 분류하여 모델을 학습시키고 평가한다.\n",
    "\n",
    "4.2 손실함수\n",
    "\n",
    "신경망 학습에서 현재의 가중치가 좋은지 안좋은지를 판별하는 지표로 손실 함수(Loss function)을 사용한다.\n",
    "\n",
    "주로 사용되는 손실함수로는 평균 제곱 오차와 교차 엔트로피 오차가 있다.\n",
    "\n",
    "손실함수라는 단어에서도 알 수 있듯이 이 함수의 값이 클수록 손실이 큰것이다.\n",
    "\n",
    "때문에 손실을 최소로 하는 지표를 찾는것이 중요하다.\n",
    "\n",
    "4.2.1 평균 제곱 오차 Mean Squared Error $\\overset{\\underset{\\mathrm{def}}{}}{=}$ MSE\n",
    "\n",
    "식 4.1 $ E = \\dfrac{1}{2} \\sum_{k=1} (y_{k}-t_{k})^2 ~~~$ :MSE 함수\n",
    "\n",
    "$ y_{k}~$ : 신경망 출력 값, $ t_{k} ~$ : 타겟 값, 정답, $k~$:데이터의 차원\n",
    "\n",
    "예를 들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.097500000000000031"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0] #cf 한 원소만 1이고 나머진 0인 배열로 나타낸 것을 원-핫 인코딩이라 함\n",
    "\n",
    "#MSE 함수 정의\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2) #보통 ^을 제곱으로 쓰는데 파이썬은 **으로...\n",
    "\n",
    "mean_squared_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제의 경우 원래 정답은 3인데 모델이 정답을 3을 0.6의 확률로 도출 했을 때의 손실함수 값을 나타낸다.\n",
    "\n",
    "만약 7일 확률을 0.6으로 도출하는 모델의 손실함수 값을 MSE로 계산해보자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59750000000000003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0, 0.05, 0.1, 0, 0.6, 0, 0]\n",
    "\n",
    "mean_squared_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 모델은 전 보다 손실함수 값이 높으므로 우리의 손실이 크다고 해석할 수 있다.\n",
    "\n",
    "그러므로 전의 모델이 더 정확한 모델이라는 판단이 가능하다.\n",
    "\n",
    "4.2.2 교차 엔트로피 오차 Cross Entropy Error $\\overset{\\underset{\\mathrm{def}}{}}{=}$ CSE\n",
    "\n",
    "식 4.2 $E= -\\sum_{k} {t}_{k} \\log {y}_{k}~~~ $  : CSE 함수\n",
    "\n",
    "$y_{k}, t_{k}$는 MSE때와 같으나 $t_{k}$의 경우에는 원-핫 인코딩의 값이다.\n",
    "\n",
    "그림 4-3 자연로그 $ y= \\log x$의 그래프\n",
    "\n",
    "$x$가 1에 가까이 갈수록 $y$값은 0에 가까워진다.\n",
    "\n",
    "때문에 정답에 해당하는 출력 ${y}_{k}$의 값이 커질수록 에러는 점점 작아지고 반대일 경우 에러값이 점점 커지는 형태의 함수이다.\n",
    "\n",
    "CSE 함수를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082545709933802"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CSE 함수 수현\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 #10^-7 인듯.\n",
    "    return -np.sum(t*np.log(y+delta)) #log(0)은 -inf 라서 그 값을 보정해주기 위한 장치\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t)) #정답에 해당하는 확률이 가장 높을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서와 마찬가지로 정답에 해당하는 확률이 낮을 때 즉, 잘못 예측했을 때에는 어떻게 되나 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025840929945458"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0, 0.05, 0.1, 0, 0.6, 0, 0]\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답을 예측 했을 때보다 함수값이 커진걸 알 수 있다. 그래서 이 함수 값이 크다는 것은 잘못예측했다고 이해할 수 있다.\n",
    "\n",
    "즉 MSE와 CSE 둘다 loss function으로서의 자격이 적합해 보인다.\n",
    "\n",
    "4.2.3 미니배치 학습\n",
    "\n",
    "우리의 목표는 훈련 데이터에 대한 손실함수 값을 구하고, 그 손실함수 값을 최소로 하는 모수(parameter,혹은 매개변수)를 찾는 것이다.\n",
    "\n",
    "배치라는 것은 앞에서도 설명했듯이 데이터를 몇개의 묶는 것이라고 했는데, 한 배치에 대해 손실함수 값은 어떻게 생각 할 수 있을까.\n",
    "\n",
    "각 데이터의 손실함수값을 다 더한 값이라고 생가하면 make sense할것 같다.\n",
    "\n",
    "배치에 대한 CSE를 생각해보자.\n",
    "\n",
    "식 4.3 $E= -\\frac{1}{N} \\sum_{n} \\sum_{k} {t}_{nk} \\log {y}_{nk}~~~ $  : 배치에 대한 손실함수\n",
    "\n",
    "단순히 하나의 데이터에 대한 4.2 식을 N개로 확장했을 뿐이다. 어렵게 생각하지 말자.\n",
    "\n",
    "그리고 총 합에다가 N으로 나눔으로써 평균손실함수값을 사용하고 있다.\n",
    "\n",
    "그 이유는 배치의 갯수에 상관없이 비교를 하기 위해서이다.(평균을 사용하지 않으면 100개의 배치보다 1000개의 배치일때가 에러함수값이 무조건 크다)\n",
    "\n",
    "일반적으로 모든 데이터에 대해 손실함수 값을 계산하지 않는다. MNIST 데이터만 해도 60,000개 인데 실제 데이터는 수백,수천만개 일때는 효율적이지 못하다.\n",
    "\n",
    "그에 대한 해결방안으로 일부의 배치에 대해 전체 값을 추정하는 방식을 사용한다.\n",
    "\n",
    "이때 일부의 배치에 해당하는 겂을 미니배치(mini-batch)라고 한다.\n",
    "\n",
    "예를 들어 MNIST 6000개의 데이터 중 무작위로 100장을 뽑아 그 100장만 사용하여 학습하는것이라고 생각할 수 있다. 이러한 방법을 미니배치 학습이라 한다.\n",
    "\n",
    "미니배치를 구현해보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shpae는 (60000, 784)\n",
      "t_train.shape는 (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir) #부모 디렉토리에 있는 파일 사용 가능??\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(\"x_train.shpae는 {}\\nt_train.shape는 {}\". format(x_train.shape,t_train.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 살펴보면, load_minist 라는 함수를 통해 MNIST 데이터를 불러왔는데, 훈련데이터와 시험데이터로 나누었고,\n",
    "\n",
    "각 데이터는 784개의 원소를 가진 배열이고, 정답 레이블은 10개의 원소를 가진 배열인데 정답에 해당하는 원소만 1이고 나머진 0인 배열이다.\n",
    "\n",
    "이제 이 데이터에서 무작위로 10장을 뽑아보려고 한다. 이런 역할을 하는 함수가 np.random.choice()이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "\n",
    "batch_mask = np.random.choice(train_size, batch_size) #train_size 중에 batch_size 갯수 만큼 숫자를 뽑아라.\n",
    "\n",
    "x_batch = x_train[batch_mask] #위에서 뽑은 수에 해당하는 index의 데이터만 갖고 온다.\n",
    "t_batch = t_train[batch_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 뽑아낸 x_batch만을 이용해서 학습을 시키는게 미니배치학습 방법이다.\n",
    "\n",
    "4.2.4 배치용 교차 엔트로피 오차 구현하기\n",
    "\n",
    "앞에서 한 개의 데이터에 해당하는 CSE(교차 엔트로피 오차)함수 를 구현했는데, 이번엔 배치에 해당하는 CSE 함수를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y))/ batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "잘 이해가 안된다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5 왜 손실함수를 설정하는가?\n",
    "\n",
    "왜 정확도라는 지표 대신에 손실함수라는 지표를 사용하는 것일까?\n",
    "\n",
    "우리는 현재의 지표가 좋은지 안좋은지의 판별을 미분을 이용해서 할건데, 이때 정확도를 지표로 사용하게 되면 대부분의 점에서 미분 값이 0이 된다.\n",
    "\n",
    "가령, 100장의 훈련 데이터를 학습시켰더니 32장을 제대로 인식했다면 정확도는 32%가 된다. 하지만 매개변수를 조정해도 정확도는 32.xx같은 연속적인 값보다는 33%, 35% 등의 불연속적인 값을 띄게 된다. 반대로 손실함수 값은 정의를 보면 모두 연속함수이기 때문에 미분값이 정확도보다는 잘 정의 될수도 있다.즉, 정확도는 매개변수의 작은 변화에 불연속적으로 변하는 반면 손실함수는 연속적으로 변한다.\n",
    "\n",
    "같은 이유로 신경망에서 활성화 함수로 step function을 잘 이용하지 않는다. 대신 부드러운 시그모이드 함수를 이용한다.\n",
    "\n",
    "그림 4-4 step 함수와 시그모이드 함수\n",
    "\n",
    "![title](/notebooks/equations_and_figures/deep_learning_images/fig%204-4.png)\n",
    "\n",
    "계단 함수는 대부분의 점에서 미분값이 0인 반면, 시그모이드 함수는 어떤 점에서도 미분값이 0이 되지 않으므로 올바르게 학습을 할 수 있다.\n",
    "\n",
    "4.3 수치 미분\n",
    "\n",
    "손실함수를 최소화 하는 점을 찾을때 미분을 이용할건데, 그전에 미분을 간단히 복습.\n",
    "\n",
    "4.3.1 미분\n",
    "\n",
    "미분이란 한순간의 변화량을 표시한 것이다. \n",
    "\n",
    "식 4.4 : 도함수의 정의 $\\dfrac{df(x)}{dx} = \\lim_{h->0} \\dfrac{f(x+h)-f(x)}{h}$\n",
    "\n",
    "미분은 $x$가 아주 조금 변할때 함수 $f(x)$는 얼마나 변하는지 보여주는 양이라고 생각하면 되겠다.\n",
    "\n",
    "미분을 구현해보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#수치 미분 함수\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h)-f(x))/h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "얼핏보면 잘 정의한것 같지만, 이코드는 두가지의 말썽을 일으킨다. \n",
    "\n",
    "첫번째로,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$10^{-50}$ 을 32비트 부동 소수점으로 나타내면 0의 값을 나타내게 된다. 그래서 실제로 이 값으로 뭔가 나누면 컴퓨터가 계산을 못할 가능성이 크다. 그래서 우리는 h=1e-4정도만 하자.\n",
    "\n",
    "두번째는 위의 함수는 본질적으로 $x+h$와 $x$사이의 기울기이다. 본질적으로 $x$에서의 기울기가 아니다.\n",
    "\n",
    "![title](data/images/fig 4-5.png)\n",
    "\n",
    "어쨋든 그 차이를 줄이려면 h에 해당하는 부분을 계속 0으로 보내야하는데, 이부분을 컴퓨터로 다루기가 힘들다.\n",
    "\n",
    "대신에 이 오차를 줄이기 위해 같은 의미이지만 다른 방법을 쓴다. \n",
    "\n",
    "바로,  $\\dfrac{df(x)}{dx} = \\lim_{h->0} \\dfrac{f(x+h)-f(x)}{h} = \\lim_{h->0} \\dfrac{f(x+h)-f(x-h)}{2h}$ 이다.\n",
    "\n",
    "두 개선사항을 반영하여 다시 미분함수를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h)-f(x-h))/ (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.2 수치 미분의 예\n",
    "\n",
    "다음과 같은 식을 수치미분을 이용하여 계산해보자. \n",
    "\n",
    "[식 4.5] $ y=0.01x^2 +0.1x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAF5CAYAAAAh0Xi4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4af53f1080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "#함수를 그려보자.\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0,20,0.1) #0에서 20까지 0.1간격의 배열\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=5에서의 미분값 0.1999999999990898\n",
      "x=10에서의 미분값 0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# x=5, x=10 일때 미분값\n",
    "print('x=5에서의 미분값 {}\\nx=10에서의 미분값 {}'.format(numerical_diff(function_1,5), numerical_diff(function_1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 값은 수치적 미분 값이고, 원래 $f(x)=0.01x^2 + 0.1x$의 도함수는 $f'(x)=0.02x+0.1$이니까 $f'(5)=0.2,~f'(10)=0.3$ 의 값과 비교해보면 차이가 얼마 나지 않는다.\n",
    "\n",
    "4.3.3 편미분\n",
    "\n",
    "지금 까지는 $f:\\mathbb{R} -> \\mathbb{R}$의 함수였다면, 이제 $f:\\mathbb{R}^2 -> \\mathbb{R}$의 함수를 생각해보자.\n",
    "\n",
    "[식 4.6] $~~~~f(x_0,x_1)={x_0}^2+{x_1}^2~~~~$을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[그림 4-8] $~~~~f(x_0,x_1)={x_0}^2+{x_1}^2~~~~$ 의 그래프\n",
    "\n",
    "![title](data/images/fig%204-8.png)\n",
    "\n",
    "이 함수의 $x_0$에 대한 편미분 $\\frac{\\partial f}{\\partial x_0}$과 $x_1$에 대한 편미분 $\\frac{\\partial f}{\\partial x_1}$ 을 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_2(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_0 = 3, x_1=4 일 때 x_0에 대한 편미분\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0+4**2\n",
    "\n",
    "numerical_diff(function_tmp1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_0 = 3, x_1 = 4 일 때 x_1에 대한 편미분\n",
    "def function_tmp2(x1):\n",
    "    return 3**2+x1*x1\n",
    "\n",
    "numerical_diff(function_tmp1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 기울기\n",
    "\n",
    "$\\mathbb{R}$에서 $\\mathbb{R}$로 가는 함수에서의 미분값은 기울기를 의미했다. 그러면 다변수일 경우에는 어떻게 생각할 수 있을까?\n",
    "\n",
    "$\\mathbb{R}^2$에서 $\\mathbb{R}$로 가는 함수에서 $(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$ 의 값을 생각해보자. 다변수 함수에서는 이 값이 기울기 벡터(Gradient)가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "식 4.6의 (3,4)에서의 gradient = [ 6.  8.]\n",
      "식 4.6의 (0,2)에서의 gradient = [    0 15000]\n",
      "식 4.6의 (3,0)에서의 gradient = [25000     0]\n"
     ]
    }
   ],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) #x와 같은 모양의 배열 생성 원소는 0으로 채워짐\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "print(\"식 4.6의 (3,4)에서의 gradient = {}\\n식 4.6의 (0,2)에서의 gradient = {}\\n식 4.6의 (3,0)에서의 gradient = {}\".format(numerical_gradient(function_2, np.array([3.0,4.0])), numerical_gradient(function_2, np.array([0,2])),numerical_gradient(function_2,np.array([3,0]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "그러면 이 각 점에서 구한 gradient값이 어떤 의미를 가지는지 생각해보자.\n",
    "\n",
    "[그림 4-9] $~~~~f(x_0,x_1)={x_0}^2+{x_1}^2~~~~$ 의 각 점에서의 gradient에 -를 붙인 값\n",
    "![](data/images/fig%204-9.png)\n",
    "$-\\nabla f(0,2) = (0,-4)$이 의미하는 바는 (0,2)에서의 함수 $f$가 가장 빨리 감소하는 방향은 (0,-4)방향이라는 뜻이다.\n",
    "\n",
    "그리고 전체 화살표방향이 (0,0)으로 향하고 있는것을 보아도 $-\\nabla$값이 함수의 감소방향을 나타냄을 알 수 있다.\n",
    "\n",
    "정확히 말하면 gradient가 가리키는 방향은 각 점에서 함수가 가장 크게 감소하는 방향이다.\n",
    "\n",
    "4.4.1 경사법(경사하강법)\n",
    "\n",
    "손실함수를 최소화시키는 최적의 매개변수를 찾는게 우리의 목표이다.\n",
    "\n",
    "하지만 어떤 매개변수가 손실함수를 최소화 하는지 찾는 방법은 쉽지 않다.\n",
    "\n",
    "이 때 각 점에서의 gradient를 이용하여 손실함수 (혹은 문제에 따라서 목적함수라고도 한다)를 최소화하는 점을 찾는 방법을 경사법(Gradient method)라고 한다.\n",
    "\n",
    "하지만 복잡한 함수일수록 gradient방향으로 따라간다고 해서 함수의 최소값을 보장해주지는 않는다. 실제로는 거의 최소값을 보장해주지 않는다.\n",
    "\n",
    "함수가 극소값, 최솟값, saddle point일 때 모두 gradient 값이 0이 되므로 ( gradient가 0이 되는 점을 수학적으로 critical point라고 한다) 기울기가 0인 점이라고 해서 반드시 그점이 최솟값이 된다는 보장을 수학적으로 할 수가 없다. 다만 함수가 convex등의 좋은 조건 속에서라면 가능한 이야기다. 그래서 실제로 gradient method가 극소값에 빠지게 되면 잘 학습이 되지 않는 경우도 있다.\n",
    "\n",
    "그래서 최솟값이면 아주 좋지만, 적어도 지금 상황보다는 나아지는 방향을 찾는것으로 만족하는 것이다.\n",
    "\n",
    "[식 4.7] gradient method\n",
    "\n",
    "$x_0 = x_0 - \\eta \\frac {\\partial f}{\\partial x_0}$\n",
    "\n",
    "$x_1 = x_1 - \\eta \\frac {\\partial f}{\\partial x_1}$\n",
    "\n",
    "$\\eta$는 줄어드는 방향으로 얼마나 앞으로 나아갈건지에 해당하는 양이라고 생각해도 되고, 신경망에서 사용하는 용어로는 학습률(Learning Rate)가 되겠다.\n",
    "\n",
    "[식 4.7]의 방법을 계속 반복하여 gradient 값이 0이거나 혹은 0과 가까울 때 멈추면 된다.\n",
    "\n",
    "이때 적절한 학습률을 정해야 하는데, 학습률에 따라서 이 방법의 성능이 좌우되기도 한다.\n",
    "\n",
    "gradient method를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad # x = x-lr*grad의 의미\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
