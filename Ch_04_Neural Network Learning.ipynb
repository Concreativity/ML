{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch_04_신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 장(3장)에서는 신경망의 가중치를 직접 설정해 둔 값으로 구현을 해보았다.\n",
    "\n",
    "하지만 이번에는 훈련데이터로부터 직접 최적의 결과를 내는 가중치 값을 찾게끔 구현할려고 한다.\n",
    "\n",
    "이때 최적의 결과라는 걸 설명해줄 손실함수(loss function)이란 것을 정의하는데,\n",
    "\n",
    "손실함수의 값이 최소가 되도록 가중치값을 조정하도록 할 것이다.\n",
    "\n",
    "이는 수학에서 최적화 문제에 해당하는데, 최적화 방법중 하나인 gradient method을 소개한다.\n",
    "\n",
    "4.1 데이터에서 학습한다!\n",
    "\n",
    "신경망의 특장점 중 하나는 데이터에 맞게 가중치 매개변수(weight parameter)를 학습한다는 점이다.\n",
    "\n",
    "딥러닝에서는 가중치의 수가 수천, 수만이기 때문에 일일이 수작업으로 지정해주는 건 불가능 하다.\n",
    "\n",
    "이번 단원에서는 MNIST의 데이터에 따라 학습하는 신경망을 구현해보자.\n",
    "\n",
    "4.1.1 데이터 주도 학습\n",
    "\n",
    "\n",
    "기본적으로 기계학습은 데이터로부터 패턴을 발견하고 학습을 함.\n",
    "\n",
    "이전의 기계학습은 데이터로 부터 특징(feature)을 추출해 내어 그 특증의 패턴을 기계학습 알고리즘으로 학습하는 방법이었다.\n",
    "\n",
    "예를 들면 숫자 이미지(그림4-1)로부터 feature를 추출하여 svm이나 k-NN 방법으로 학습을 하는 것이다.\n",
    "\n",
    "하지만 svm이나 k-NN같은 학습을 기계가 하는 부분이라고 하면, 데이터로부터 적절한 feature를 추출하는 것은 사람의 개입하는 정도가 크다.\n",
    "\n",
    "하지만 신경망이나 딥러닝의 경우에는 사람의 개입이 없는(그래서 end-to-end machine learning이라고 불림) 기계학습 방법이다.\n",
    "\n",
    "이런 방법의 이점은 어떤 데이터이든지 같은 맥락에서 기계학습이 가능하다.\n",
    "\n",
    "(이전의 경우에는 숫자데이터에 알맞은 feature, 강아지 사진에 알맞는 feature를 사람이 따로 고안을 했어야 함)\n",
    "\n",
    "그래서 우리는 현재까진 흠이 없어보이는 이 신경망에 대해서 좀 더 알아보자.\n",
    "\n",
    "4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    "신경망을 설명하기 앞서 기계학습 데이터 취급시 주의할 점을 보자.\n",
    "\n",
    "기계학습의 데이터는 training(훈련),test(시험)데이터 2가지로 분류한다.\n",
    "\n",
    "훈련 데이터로 기계학습을 시켜 최적의 가중치를 찾아낸 다음에, 시험 데이터로 그 모델의 성능(범용 능력이라 함)을 평가하는 것이다.\n",
    "\n",
    "그러면 왜 이렇게 학습데이터를 분류를 해야하는가?\n",
    "\n",
    "모델의 궁극적인 목표는 새로운 데이터에 대해 어떤 성능을 발휘하는 가이다.\n",
    "\n",
    "그런데 학습데이터에만 잘 들어맞는 모델은 실제로 쓸모가 없다. 또 학습데이터에만 지나치게 모델이 잘 들어맞는 것을 overfitting(과적합)이라고 한다.\n",
    "\n",
    "이 오버피팅을 피하기 위해 학습데이터를 분류하여 모델을 학습시키고 평가한다.\n",
    "\n",
    "4.2 손실함수\n",
    "\n",
    "신경망 학습에서 현재의 가중치가 좋은지 안좋은지를 판별하는 지표로 손실 함수(Loss function)을 사용한다.\n",
    "\n",
    "주로 사용되는 손실함수로는 평균 제곱 오차와 교차 엔트로피 오차가 있다.\n",
    "\n",
    "손실함수라는 단어에서도 알 수 있듯이 이 함수의 값이 클수록 손실이 큰것이다.\n",
    "\n",
    "때문에 손실을 최소로 하는 지표를 찾는것이 중요하다.\n",
    "\n",
    "4.2.1 평균 제곱 오차 Mean Squared Error $\\overset{\\underset{\\mathrm{def}}{}}{=}$ MSE\n",
    "\n",
    "식 4.1 $ E = \\dfrac{1}{2} \\sum_{k=1} (y_{k}-t_{k})^2 ~~~$ :MSE 함수\n",
    "\n",
    "$ y_{k}~$ : 신경망 출력 값, $ t_{k} ~$ : 타겟 값, 정답, $k~$:데이터의 차원\n",
    "\n",
    "예를 들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.097500000000000031"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0] #cf 한 원소만 1이고 나머진 0인 배열로 나타낸 것을 원-핫 인코딩이라 함\n",
    "\n",
    "#MSE 함수 정의\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2) #보통 ^을 제곱으로 쓰는데 파이썬은 **으로...\n",
    "\n",
    "mean_squared_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제의 경우 원래 정답은 3인데 모델이 정답을 3을 0.6의 확률로 도출 했을 때의 손실함수 값을 나타낸다.\n",
    "\n",
    "만약 7일 확률을 0.6으로 도출하는 모델의 손실함수 값을 MSE로 계산해보자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59750000000000003"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0, 0.05, 0.1, 0, 0.6, 0, 0]\n",
    "\n",
    "mean_squared_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 모델은 전 보다 손실함수 값이 높으므로 우리의 손실이 크다고 해석할 수 있다.\n",
    "\n",
    "그러므로 전의 모델이 더 정확한 모델이라는 판단이 가능하다.\n",
    "\n",
    "4.2.2 교차 엔트로피 오차 Cross Entropy Error $\\overset{\\underset{\\mathrm{def}}{}}{=}$ CSE\n",
    "\n",
    "식 4.2 $E= -\\sum_{k} {t}_{k} \\log {y}_{k}~~~ $  : CSE 함수\n",
    "\n",
    "$y_{k}, t_{k}$는 MSE때와 같으나 $t_{k}$의 경우에는 원-핫 인코딩의 값이다.\n",
    "\n",
    "그림 4-3 자연로그 $ y= \\log x$의 그래프\n",
    "\n",
    "$x$가 1에 가까이 갈수록 $y$값은 0에 가까워진다.\n",
    "\n",
    "때문에 정답에 해당하는 출력 ${y}_{k}$의 값이 커질수록 에러는 점점 작아지고 반대일 경우 에러값이 점점 커지는 형태의 함수이다.\n",
    "\n",
    "CSE 함수를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082545709933802"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CSE 함수 수현\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 #10^-7 인듯.\n",
    "    return -np.sum(t*np.log(y+delta)) #log(0)은 -inf 라서 그 값을 보정해주기 위한 장치\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t)) #정답에 해당하는 확률이 가장 높을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서와 마찬가지로 정답에 해당하는 확률이 낮을 때 즉, 잘못 예측했을 때에는 어떻게 되나 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025840929945458"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0, 0.05, 0.1, 0, 0.6, 0, 0]\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답을 예측 했을 때보다 함수값이 커진걸 알 수 있다. 그래서 이 함수 값이 크다는 것은 잘못예측했다고 이해할 수 있다.\n",
    "\n",
    "즉 MSE와 CSE 둘다 loss function으로서의 자격이 적합해 보인다.\n",
    "\n",
    "4.2.3 미니배치 학습\n",
    "\n",
    "우리의 목표는 훈련 데이터에 대한 손실함수 값을 구하고, 그 손실함수 값을 최소로 하는 모수(parameter,혹은 매개변수)를 찾는 것이다.\n",
    "\n",
    "배치라는 것은 앞에서도 설명했듯이 데이터를 몇개의 묶는 것이라고 했는데, 한 배치에 대해 손실함수 값은 어떻게 생각 할 수 있을까.\n",
    "\n",
    "각 데이터의 손실함수값을 다 더한 값이라고 생가하면 make sense할것 같다.\n",
    "\n",
    "배치에 대한 CSE를 생각해보자.\n",
    "\n",
    "식 4.3 $E= -\\frac{1}{N} \\sum_{n} \\sum_{k} {t}_{nk} \\log {y}_{nk}~~~ $  : 배치에 대한 손실함수\n",
    "\n",
    "단순히 하나의 데이터에 대한 4.2 식을 N개로 확장했을 뿐이다. 어렵게 생각하지 말자.\n",
    "\n",
    "그리고 총 합에다가 N으로 나눔으로써 평균손실함수값을 사용하고 있다.\n",
    "\n",
    "그 이유는 배치의 갯수에 상관없이 비교를 하기 위해서이다.(평균을 사용하지 않으면 100개의 배치보다 1000개의 배치일때가 에러함수값이 무조건 크다)\n",
    "\n",
    "일반적으로 모든 데이터에 대해 손실함수 값을 계산하지 않는다. MNIST 데이터만 해도 60,000개 인데 실제 데이터는 수백,수천만개 일때는 효율적이지 못하다.\n",
    "\n",
    "그에 대한 해결방안으로 일부의 배치에 대해 전체 값을 추정하는 방식을 사용한다.\n",
    "\n",
    "이때 일부의 배치에 해당하는 겂을 미니배치(mini-batch)라고 한다.\n",
    "\n",
    "예를 들어 MNIST 6000개의 데이터 중 무작위로 100장을 뽑아 그 100장만 사용하여 학습하는것이라고 생각할 수 있다. 이러한 방법을 미니배치 학습이라 한다.\n",
    "\n",
    "미니배치를 구현해보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shpae는 (60000, 784)\n",
      "t_train.shape는 (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir) #부모 디렉토리에 있는 파일 사용 가능??\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(\"x_train.shpae는 {}\\nt_train.shape는 {}\". format(x_train.shape,t_train.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 살펴보면, load_minist 라는 함수를 통해 MNIST 데이터를 불러왔는데, 훈련데이터와 시험데이터로 나누었고,\n",
    "\n",
    "각 데이터는 784개의 원소를 가진 배열이고, 정답 레이블은 10개의 원소를 가진 배열인데 정답에 해당하는 원소만 1이고 나머진 0인 배열이다.\n",
    "\n",
    "이제 이 데이터에서 무작위로 10장을 뽑아보려고 한다. 이런 역할을 하는 함수가 np.random.choice()이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "\n",
    "batch_mask = np.random.choice(train_size, batch_size) #train_size 중에 batch_size 갯수 만큼 숫자를 뽑아라.\n",
    "\n",
    "x_batch = x_train[batch_mask] #위에서 뽑은 수에 해당하는 index의 데이터만 갖고 온다.\n",
    "t_batch = t_train[batch_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 뽑아낸 x_batch만을 이용해서 학습을 시키는게 미니배치학습 방법이다.\n",
    "\n",
    "4.2.4 배치용 교차 엔트로피 오차 구현하기\n",
    "\n",
    "앞에서 한 개의 데이터에 해당하는 CSE(교차 엔트로피 오차)함수 를 구현했는데, 이번엔 배치에 해당하는 CSE 함수를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y))/ batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "잘 이해가 안된다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5 왜 손실함수를 설정하는가?\n",
    "\n",
    "왜 정확도라는 지표 대신에 손실함수라는 지표를 사용하는 것일까?\n",
    "\n",
    "우리는 현재의 지표가 좋은지 안좋은지의 판별을 미분을 이용해서 할건데, 이때 정확도를 지표로 사용하게 되면 대부분의 점에서 미분 값이 0이 된다.\n",
    "\n",
    "가령, 100장의 훈련 데이터를 학습시켰더니 32장을 제대로 인식했다면 정확도는 32%가 된다. 하지만 매개변수를 조정해도 정확도는 32.xx같은 연속적인 값보다는 33%, 35% 등의 불연속적인 값을 띄게 된다. 반대로 손실함수 값은 정의를 보면 모두 연속함수이기 때문에 미분값이 정확도보다는 잘 정의 될수도 있다.즉, 정확도는 매개변수의 작은 변화에 불연속적으로 변하는 반면 손실함수는 연속적으로 변한다.\n",
    "\n",
    "같은 이유로 신경망에서 활성화 함수로 step function을 잘 이용하지 않는다. 대신 부드러운 시그모이드 함수를 이용한다.\n",
    "\n",
    "그림 4-4 step 함수와 시그모이드 함수\n",
    "\n",
    "![title](data/images/fig%204-4.png)\n",
    "\n",
    "계단 함수는 대부분의 점에서 미분값이 0인 반면, 시그모이드 함수는 어떤 점에서도 미분값이 0이 되지 않으므로 올바르게 학습을 할 수 있다.\n",
    "\n",
    "4.3 수치 미분\n",
    "\n",
    "손실함수를 최소화 하는 점을 찾을때 미분을 이용할건데, 그전에 미분을 간단히 복습.\n",
    "\n",
    "4.3.1 미분\n",
    "\n",
    "미분이란 한순간의 변화량을 표시한 것이다. \n",
    "\n",
    "식 4.4 : 도함수의 정의 $\\dfrac{df(x)}{dx} = \\lim_{h->0} \\dfrac{f(x+h)-f(x)}{h}$\n",
    "\n",
    "미분은 $x$가 아주 조금 변할때 함수 $f(x)$는 얼마나 변하는지 보여주는 양이라고 생각하면 되겠다.\n",
    "\n",
    "미분을 구현해보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#수치 미분 함수\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h)-f(x))/h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "얼핏보면 잘 정의한것 같지만, 이코드는 두가지의 말썽을 일으킨다. \n",
    "\n",
    "첫번째로,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$10^{-50}$ 을 32비트 부동 소수점으로 나타내면 0의 값을 나타내게 된다. 그래서 실제로 이 값으로 뭔가 나누면 컴퓨터가 계산을 못할 가능성이 크다. 그래서 우리는 h=1e-4정도만 하자.\n",
    "\n",
    "두번째는 위의 함수는 본질적으로 $x+h$와 $x$사이의 기울기이다. 본질적으로 $x$에서의 기울기가 아니다.\n",
    "\n",
    "![title](data/images/fig 4-5.png)\n",
    "\n",
    "어쨋든 그 차이를 줄이려면 h에 해당하는 부분을 계속 0으로 보내야하는데, 이부분을 컴퓨터로 다루기가 힘들다.\n",
    "\n",
    "대신에 이 오차를 줄이기 위해 같은 의미이지만 다른 방법을 쓴다. \n",
    "\n",
    "바로,  $\\dfrac{df(x)}{dx} = \\lim_{h->0} \\dfrac{f(x+h)-f(x)}{h} = \\lim_{h->0} \\dfrac{f(x+h)-f(x-h)}{2h}$ 이다.\n",
    "\n",
    "두 개선사항을 반영하여 다시 미분함수를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h)-f(x-h))/ (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.2 수치 미분의 예\n",
    "\n",
    "다음과 같은 식을 수치미분을 이용하여 계산해보자. \n",
    "\n",
    "[식 4.5] $ y=0.01x^2 +0.1x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAF5CAYAAAAh0Xi4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3662986358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "#함수를 그려보자.\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0,20,0.1) #0에서 20까지 0.1간격의 배열\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=5에서의 미분값 0.1999999999990898\n",
      "x=10에서의 미분값 0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# x=5, x=10 일때 미분값\n",
    "print('x=5에서의 미분값 {}\\nx=10에서의 미분값 {}'.format(numerical_diff(function_1,5), numerical_diff(function_1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 값은 수치적 미분 값이고, 원래 $f(x)=0.01x^2 + 0.1x$의 도함수는 $f'(x)=0.02x+0.1$이니까 $f'(5)=0.2,~f'(10)=0.3$ 의 값과 비교해보면 차이가 얼마 나지 않는다.\n",
    "\n",
    "4.3.3 편미분\n",
    "\n",
    "지금 까지는 $f:\\mathbb{R} -> \\mathbb{R}$의 함수였다면, 이제 $f:\\mathbb{R}^2 -> \\mathbb{R}$의 함수를 생각해보자.\n",
    "\n",
    "[식 4.6] $~~~~f(x_0,x_1)={x_0}^2+{x_1}^2~~~~$을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[그림 4-8] $~~~~f(x_0,x_1)={x_0}^2+{x_1}^2~~~~$ 의 그래프\n",
    "\n",
    "![title](data/images/fig%204-8.png)\n",
    "\n",
    "이 함수의 $x_0$에 대한 편미분 $\\frac{\\partial f}{\\partial x_0}$과 $x_1$에 대한 편미분 $\\frac{\\partial f}{\\partial x_1}$ 을 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_2(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_0 = 3, x_1=4 일 때 x_0에 대한 편미분\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0+4**2\n",
    "\n",
    "numerical_diff(function_tmp1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_0 = 3, x_1 = 4 일 때 x_1에 대한 편미분\n",
    "def function_tmp2(x1):\n",
    "    return 3**2+x1*x1\n",
    "\n",
    "numerical_diff(function_tmp1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 기울기\n",
    "\n",
    "$\\mathbb{R}$에서 $\\mathbb{R}$로 가는 함수에서의 미분값은 기울기를 의미했다. 그러면 다변수일 경우에는 어떻게 생각할 수 있을까?\n",
    "\n",
    "$\\mathbb{R}^2$에서 $\\mathbb{R}$로 가는 함수에서 $(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$ 의 값을 생각해보자. 다변수 함수에서는 이 값이 기울기 벡터(Gradient)가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "식 4.6의 (3,4)에서의 gradient = [ 6.  8.]\n",
      "식 4.6의 (0,2)에서의 gradient = [ 0.  4.]\n",
      "식 4.6의 (3,0)에서의 gradient = [ 6.  0.]\n"
     ]
    }
   ],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) #x와 같은 모양의 배열 생성 원소는 0으로 채워짐\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "print(\"식 4.6의 (3,4)에서의 gradient = {}\\n식 4.6의 (0,2)에서의 gradient = {}\\n식 4.6의 (3,0)에서의 gradient = {}\".format(numerical_gradient(function_2, np.array([3.0,4.0])), numerical_gradient(function_2, np.array([0,2.0])),numerical_gradient(function_2,np.array([3.0,0]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "위의 코드에서 주의할점 하나만 살펴보자.\n",
    "\n",
    "numerical_gradient(function_2, np.array([3.0,4.0]))에서 np.array([3.0,4.0])대신에 np.array([3,4])를 대입하면 적절한 값이 나오지 않는다.\n",
    "\n",
    "왜냐하면, x=np.array([3,4])로 정의하면 x는 int형 변수를 가지게 되는데 이 int형 변수는 무조건 버림을 하기 때문에, x[0]-h를 하게되면 \n",
    "\n",
    "실제 값은 3-0.0001이지만 출력되는 값은 2로 나오기 때문에 에러가 생긴다. 때문에 반드시  np.array([3.0,4.0])을 대입해주어야 한다.\n",
    "\n",
    "그러면 이 각 점에서 구한 gradient값이 어떤 의미를 가지는지 생각해보자.\n",
    "\n",
    "[그림 4-9] $~~~~f(x_0,x_1)={x_0}^2+{x_1}^2~~~~$ 의 각 점에서의 gradient에 -를 붙인 값\n",
    "![](data/images/fig%204-9.png)\n",
    "$-\\nabla f(0,2) = (0,-4)$이 의미하는 바는 (0,2)에서의 함수 $f$가 가장 빨리 감소하는 방향은 (0,-4)방향이라는 뜻이다.\n",
    "\n",
    "그리고 전체 화살표방향이 (0,0)으로 향하고 있는것을 보아도 $-\\nabla$값이 함수의 감소방향을 나타냄을 알 수 있다.\n",
    "\n",
    "정확히 말하면 gradient가 가리키는 방향은 각 점에서 함수가 가장 크게 감소하는 방향이다.\n",
    "\n",
    "4.4.1 경사법(경사하강법)\n",
    "\n",
    "손실함수를 최소화시키는 최적의 매개변수를 찾는게 우리의 목표이다.\n",
    "\n",
    "하지만 어떤 매개변수가 손실함수를 최소화 하는지 찾는 방법은 쉽지 않다.\n",
    "\n",
    "이 때 각 점에서의 gradient를 이용하여 손실함수 (혹은 문제에 따라서 목적함수라고도 한다)를 최소화하는 점을 찾는 방법을 경사법(Gradient method)라고 한다.\n",
    "\n",
    "하지만 복잡한 함수일수록 gradient방향으로 따라간다고 해서 함수의 최소값을 보장해주지는 않는다. 실제로는 거의 최소값을 보장해주지 않는다.\n",
    "\n",
    "함수가 극소값, 최솟값, saddle point일 때 모두 gradient 값이 0이 되므로 ( gradient가 0이 되는 점을 수학적으로 critical point라고 한다) 기울기가 0인 점이라고 해서 반드시 그점이 최솟값이 된다는 보장을 수학적으로 할 수가 없다. 다만 함수가 convex등의 좋은 조건 속에서라면 가능한 이야기다. 그래서 실제로 gradient method가 극소값에 빠지게 되면 잘 학습이 되지 않는 경우도 있다.\n",
    "\n",
    "그래서 최솟값이면 아주 좋지만, 적어도 지금 상황보다는 나아지는 방향을 찾는것으로 만족하는 것이다.\n",
    "\n",
    "[식 4.7] gradient method\n",
    "\n",
    "$x_0 = x_0 - \\eta \\frac {\\partial f}{\\partial x_0}$\n",
    "\n",
    "$x_1 = x_1 - \\eta \\frac {\\partial f}{\\partial x_1}$\n",
    "\n",
    "$\\eta$는 줄어드는 방향으로 얼마나 앞으로 나아갈건지에 해당하는 양이라고 생각해도 되고, 신경망에서 사용하는 용어로는 학습률(Learning Rate)가 되겠다.\n",
    "\n",
    "[식 4.7]의 방법을 계속 반복하여 gradient 값이 0이거나 혹은 0과 가까울 때 멈추면 된다.\n",
    "\n",
    "이때 적절한 학습률을 정해야 하는데, 학습률에 따라서 이 방법의 성능이 좌우되기도 한다.\n",
    "\n",
    "gradient method를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad # x = x-lr*grad의 의미\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "인수 f는 최적화 하려는 함수 (수학용어로는 Objective function, 목적함수), init_x는 초기 값(initial value), lr은 learning rate, step_num은 반복횟수를 의미한다.\n",
    "\n",
    "예제를 보자.\n",
    "\n",
    "$f(x_0 , x_1)={x_{0}}^2 + {x_{1}}^2 $ 의 최솟값을 구하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0]) #시작점이 (-3,4)\n",
    "\n",
    "gradient_descent(function_2,init_x=init_x,lr=0.1,step_num=100) #(0,0)에 가깝게 나와야 정상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 $f(x_0 , x_1)={x_{0}}^2 + {x_{1}}^2 $의 그래프를 봐도 알 수 있듯이, Global minimum은 (0,0)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAFyCAYAAACtP0M/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c2eacfac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#점의 자취를 확인해보자.\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from ch04.gradient_2d import numerical_gradient\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습률에 따른 gradient descent method를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.58983747e+13,  -1.29524862e+12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 큰 예 lr=10.0\n",
    "init_x = np.array([-3.0,4.0])\n",
    "\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad # x = x-lr*grad의 의미\n",
    "    return x\n",
    "\n",
    "gradient_descent(function_2,init_x=init_x,lr=10.0,step_num=100) #발산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 낮은 예 lr=1e-10\n",
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2,init_x=init_x,lr=1e-10,step_num=100) #초기값과 별 차이가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 gradient method의 learning late를 hyperparameter(초 매개변수)라고 한다.\n",
    "\n",
    "신경망에서 가중치와 편향의 값은 신경망이 스스로 학습하면서 최적의 값을 찾는 반면,\n",
    "\n",
    "learning late같은 변수는 사람이 직접 입력해주어야 하기 때문에 이름을 달리 부르는것 같다.\n",
    "\n",
    "4.4.2 신경망에서의 기울기\n",
    "\n",
    "그렇다면 본론으로 돌아와서 신경망에서는 어떤 기울기로 움직여서 목적함수인 손실함수의 값을 최소로 만들어갈것인지 생각해봐야 한다.\n",
    "\n",
    "이 때 변하는 값은 신경망의 각 가중치이니까 기울기는 신경망 가중치에 대한 손실함수의 값이 자연스럽다.\n",
    "\n",
    "가중치 행렬을 $\\mathbf{W} = \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \\end{pmatrix}$, 손실함수 $L$라 할 때 $\\mathbf{W}$에 대한 $L$의 기울기는 $\\frac{\\partial L}{\\partial \\mathbf{W}} = \\begin{pmatrix} \\frac{\\partial L}{\\partial w_{11}} &  \\frac{\\partial L}{\\partial w_{12}} \\\\  \\frac{\\partial L}{\\partial w_{21}} &  \\frac{\\partial L}{\\partial w_{22}} \\\\  \\frac{\\partial L}{\\partial w_{31}} &  \\frac{\\partial L}{\\partial w_{32}} \\end{pmatrix}$ 가 된다 [식4.8]\n",
    "\n",
    "$L$이 실수로가는 함수이기 때문에 행렬에 대한 미분을 각각 성분별로의 미분으로 생각하면 된다.\n",
    "\n",
    "그럼 간단히 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.77081736 -0.72651719]\n",
      " [-1.23090421  1.27162104]\n",
      " [ 0.10364353 -0.59330697]]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(3,2) # 정규분포로 초기화, 정규분포에서 뽑은 수로 이루어진 2x3행렬\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(self.W,x)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss       \n",
    "    \n",
    "net = simpleNet()\n",
    "print(net.W) #초기 가중치 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.71635589  0.4059164  -0.47179015]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p) # x에 가중치를 곱해서 모두 더한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3065590204961652"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x,t) #x값에 가중치 곱함 -> p값 -> 활성화함수에 대입한 값과 정답 레이블과 비교한 교차엔트로피 에러 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 가중치 값을 변화시키면 이 손실함수값이 어떻게 변하는지 확인하기 위해 기울기를 계산해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04679634  0.0701945 ]\n",
      " [ 0.3907536   0.58613041]\n",
      " [-0.43754994 -0.65632491]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dw = numerical_gradient(f, net.W) #여기서 net.W는 위에서 선언된 값임.\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 행렬을 해석할 때, $w_{11}$이 0.5라고 한다면 (위에서 W를 랜덤으로 가져왔기 때문에 시행할때마다 값이 달라진다) $w_{11}$을 $h$만큼 증가시키면 손실함수의 값은 0.5$h$만큼 증가한다는 뜻이다. 그래서 실제 처음 설정된 $W$에 대해 이 행렬이 계산이되면 어떤 값을 조절해야 손실함수 값이 크게 감소할 수 있을지 알 수 있다.\n",
    "\n",
    "이제 이 gradient로 앞서 했던 gradient method를 똑같이 실행하면 된다.\n",
    "\n",
    "4.5 학습 알고리즘 구현\n",
    "\n",
    "지금까지 배운 키워드를 보면 '손실함수','미니배치', '기울기', '경사하강법' 등이 있다. 신경망 학습과정을 간단히 복습해보자.\n",
    "\n",
    "전제\n",
    "\n",
    "$~~~$ 신경망에 모수로는 가중치와 편향이 있고, 훈련데이터를 통해 손실함수를 최소화 하도록 가중치와 편향을 조정하는 것을 '학습'이라고 한다.\n",
    "\n",
    "1단계-미니배치\n",
    "\n",
    "$~~~$ 훈련 데이터가 많은 경우, 데이터 중 일부를 무작위로 추출한다. 이 추출한 데이터를 미니배치라고 하고 앞으로 이 미니배치에 대해 손실함수 값을 줄일 거다.\n",
    "\n",
    "2단계-기울기 산출\n",
    "\n",
    "$~~~$ 손실함수의 값을 줄이기 위해 각 가중치 매개변수에 대한 손실함수 기울기를 계산한다. 이 값은 손실함수를 작게하는 방향을 알려준다.\n",
    "\n",
    "3단계-매개변수 갱신\n",
    "\n",
    "$~~~$ 가중치 매개변수를 기울기 방향으로 조금 갱신.\n",
    "\n",
    "4단계-반복\n",
    "\n",
    "$~~~$ 적절한 criterion이 만족할때 까지 1~3단계를 반복한다.\n",
    "\n",
    "이 방법은 미니배치를 결정할 때 랜덤하게 결정되므로 확률적 경사 하강법 (Stochastic Gradient Descent$\\overset{\\underset{\\mathrm{def}}{}}{=}$SGD)라고 한다.\n",
    "\n",
    "실제 MNIST 데이터셋을 이용해서 구현해보자.\n",
    "\n",
    "4.5.1 2층 신경망 클래스 구현\n",
    "\n",
    "우선 2층 신경망을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2층 신경망\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    #numerical_gradient가 시간이 굉장히 오래걸릴수 있기 때문에 다음의 오차역전파법 코드를 사용해본다.코드 설명은 나중에 다시.\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class에 대한 설명은 파이썬 부분에서 살펴보도록 하고, 앞에서 step by step으로 했던 코드들을 모두 한군데에 묶어놓은거라고 생각하면 된다.\n",
    "\n",
    "TwoLayerNet클래스의 변수에 대해 한번 살펴보자.\n",
    "\n",
    "params : 신경망의 매개변수를 보관하는 인스턴스 변수 params['W1']은 1번째 층의 가중치이다.\n",
    "\n",
    "grads :  기울기를 보관하는 변수\n",
    "\n",
    "TwoLayerNet 클래스의 각 함수를 보자.\n",
    "\n",
    "__init__ : 초기화를 수행하는 부분이다. 클래스의 인수는 input_size(입력뉴런 수), hidden_size(은닉층의 뉴런수), outputsize(출력중의 뉴련수)\n",
    "\n",
    "predict(self,x) : 주어진 신경망으로 추론한 예측값\n",
    "\n",
    "loss(self,x,t) : 예측한값과 주어진 정답레이블을 이용한 손실함수\n",
    "\n",
    "accuracy(self,x,t) : 정확도 계산\n",
    "\n",
    "numerical_gradient(self,x,t) : 각 가중치에 대한 기울기\n",
    "\n",
    "이해가 잘 안되니 예를 들어보자.\n",
    "\n",
    "TwolayerNet 클래스의 딕셔너리 변수인 params에 잘 정의가 되는지 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784,hidden_size=100,output_size=10)\n",
    "net.params['W1'].shape #784x100행렬이 나와야 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "이번엔 TwoLayerNet 클래스 속의 predict 기능이 잘 정의 되어있는지 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100,784) # 784 feature를 가진 100개의 데이터 생각, 즉 배치가 100\n",
    "y = net.predict(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 grads변수에 기울기가 잘 저장이 되었는지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads['W1'].shape는 (784, 100)\n",
      "grads['b1'].shape는 (100,)\n",
      "grads['W2'].shape는 (100, 10)\n",
      "grads['b2'].shape는 (10,)\n"
     ]
    }
   ],
   "source": [
    "###시간이 오래 걸림###\n",
    "x = np.random.rand(100,784)\n",
    "t = np.random.rand(100,10)\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "print(\"grads['W1'].shape는 {}\\ngrads['b1'].shape는 {}\\ngrads['W2'].shape는 {}\\ngrads['b2'].shape는 {}\".format(grads['W1'].shape,grads['b1'].shape,grads['W2'].shape,grads['b2'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads['W1'].shape는 (784, 100)\n",
      "grads['b1'].shape는 (100,)\n",
      "grads['W2'].shape는 (100, 10)\n",
      "grads['b2'].shape는 (10,)\n"
     ]
    }
   ],
   "source": [
    "#오차역전파법을 사용한 기울기\n",
    "\n",
    "grads=  net.gradient(x,t)\n",
    "print(\"grads['W1'].shape는 {}\\ngrads['b1'].shape는 {}\\ngrads['W2'].shape는 {}\\ngrads['b2'].shape는 {}\".format(grads['W1'].shape,grads['b1'].shape,grads['W2'].shape,grads['b2'].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from ch04.two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list=[]\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치는 100개로 설정했다. 즉 60,000개의 데이터 중에서 100개를 임의로 추출하여 그 데이터들에 대해 신경망을 학습했다. 갱신할 때마다 손실함수의 값을 train_loss_list에 저장했다. 즉 train_loss_list에는 각 단계별 손실함수 값이 저장되어있다.\n",
    "\n",
    "[그림 4-11] 손실함수 값의 추이\n",
    "\n",
    "![](data/images/fig%204-11.png)\n",
    "\n",
    "그림에서와 같이 반복횟수가 증가함에 따라 손실함수 값이 줄어든다는 것은 학습이 잘 되고 있다는 뜻이다.\n",
    "\n",
    "4.5.3 시험 데이터로 평가\n",
    "\n",
    "그림 4-11에서 처럼 손실함수 값을 줄여주는 최적의 가중치들을 찾았는데, 과연 이 가중치들이 다른 데이터에 대해서도 잘 적용이 될지 확인을 해봐야 한다. 즉 훈련데이터에 오버피팅이 일어난건 아닌지 확인을 해봐야한다.\n",
    "\n",
    "여기서는 학습 도중 시험데이터를 대상으로 정확도를 기록하도록 할것이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
